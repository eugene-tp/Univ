{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as utils \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the Image\n",
    "CONTENT_IMG = \"content.jpg\"\n",
    "STYLE_IMG = \"style.jpg\"\n",
    "\n",
    "STEPS = 500\n",
    "\n",
    "IMSIZE = 512 if torch.cuda.is_available() else 128\n",
    "\n",
    "MEAN=[0.485, 0.456, 0.406]\n",
    "STD=[0.229, 0.224, 0.225]\n",
    "\n",
    "CNTWEIGHT = 1\n",
    "STLWEIGHT = 1000\n",
    "\n",
    "#etc\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = models.vgg19(pretrained=True).features.to(device).eval() #평가판을 사용해야 모델에 영향을 안주고 할 수 있다...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Images\n",
    "convert = transforms.Compose([ #데이터셋을 가져올 때 형태를 변환해주는 부분.\n",
    "    transforms.Resize(IMSIZE), #이미지 크기 변환\n",
    "    transforms.ToTensor() #pytorch에서 사용하기 위한 tensor자료구조로 변환\n",
    "     #받아오는 데이터를 노말라이징.(특정 부분이 너무 어둡거나 밝은 경우 데이터가 튀는 현상을 방지)\n",
    "])\n",
    "reconvert = transforms.ToPILImage()\n",
    "\n",
    "def imageLoad(image_name):  #이미지를 텐서로 전환해 모델에 \n",
    "    img = Image.open(image_name)\n",
    "    img = convert(img).unsqueeze(0).clone()\n",
    "    return img.to(device, torch.float)\n",
    "\n",
    "def showImage(tensor): #결과를 보여주기 위한 작업\n",
    "    image = tensor.cpu().clone() #원본이미지 손상 방지용 clone\n",
    "    image = image.squeeze(0) #delete fake dimension\n",
    "    image = reconvert(image)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "style_img = imageLoad(STYLE_IMG) \n",
    "content_img = imageLoad(CONTENT_IMG)\n",
    "#plt.figure()\n",
    "#showImage(style_img)\n",
    "#plt.figure()\n",
    "#showImage(content_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "w = 0.2 #스타일 각 계층에 대한 가중치 = 1/5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.randn(content_img.data.size(), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContentLoss(y, y_pred):\n",
    "    \n",
    "    loss = F.mse_loss(y,y_pred) #/2.0\n",
    "    return loss\n",
    "\n",
    "def gramMatrix(input):\n",
    "    b, c, h, w = input.size() #batch size(1), chanel, height, width\n",
    "    features = input.view(b*c, h*w) \n",
    "    gram = torch.mm(features, features.t())\n",
    "    return gram\n",
    "\n",
    "def StyleLoss(y_pred, y,layer):\n",
    "    \n",
    "    N = layer.out_channels #the number of feature maps at layer L -> 채널 수\n",
    "    M = layer.kernel_size[0] #height * width of feature maps at layer L\n",
    "    A = gramMatrix(y).detach() #original list\n",
    "    G = gramMatrix(y_pred).detach() #generated list\n",
    "    \n",
    "    E = F.mse_loss(G,A)\n",
    "    #E = (1/4* N*N*M*M) * F.mse_loss(G,A) \n",
    "    return E  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(vgg, style_img, content_img, input_img):\n",
    "    \n",
    "    normalization = Normalization(MEAN, STD).to(device)\n",
    "    \n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    i=0\n",
    "    for layer in vgg.children():  \n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "            \n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "            #layer = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "            \n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "    \n",
    "        \n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = vgg(content_img).detach()\n",
    "            ans = vgg(input_img).detach()\n",
    "            content_loss = ContentLoss(target, ans)\n",
    "            content_losses.append(content_loss)    \n",
    "        \n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = vgg(style_img).detach()\n",
    "            ans_feature = vgg(input_img).detach()\n",
    "            style_loss = StyleLoss(target_feature, ans_feature, layer)\n",
    "            style_losses.append(style_loss)\n",
    "        \n",
    "    return style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOutput(vgg, content_img, style_img, input_img):\n",
    "    \n",
    "    style_losses, content_losses = get_losses(vgg,\n",
    "                                              style_img, content_img, input_img)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "    \n",
    "    num = 0\n",
    "    print('Optimizing..')\n",
    "    while num <= STEPS:\n",
    "        \n",
    "        def closure():\n",
    "            # 입력 이미지의 업데이트된 값들을 보정합니다\n",
    "            input_img.data.clamp_(0, 1)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            vgg(input_img)\n",
    "\n",
    "            style_total = 0\n",
    "            content_total = 0\n",
    "            \n",
    "            for i in content_losses:\n",
    "                content_total += i\n",
    "            for i in style_losses:\n",
    "                style_total += i #*w\n",
    "                \n",
    "            total_loss = style_total * STLWEIGHT + content_total * CNTWEIGHT\n",
    "            \n",
    "            print(\"total:\",total_loss)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            num += 1\n",
    "            if num % 50 == 0:\n",
    "                print(\"run {}:\".format(num))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(style_total.item(), content_total.item()))\n",
    "                print()\n",
    "            return style_total + content_total\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "    input_img.data.clamp_(0, 1)\n",
    "    \n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing..\n",
      "total: tensor(1.0505e+09)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-85afbf68e39b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshowImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-e6a731a96aa3>\u001b[0m in \u001b[0;36mmakeOutput\u001b[0;34m(vgg, content_img, style_img, input_img)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstyle_total\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontent_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0minput_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-e6a731a96aa3>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "result_img = makeOutput(vgg, content_img, style_img, input_img)\n",
    "\n",
    "plt.figure()\n",
    "showImage(result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # 그라디언트를 동적으로 계산하는 데 사용되는 트리에서 대상 콘텐츠를 '분리' 합니다.\n",
    "        # :이 값은 변수(variable)가 아니라 명시된 값입니다.\n",
    "        # 그렇지 않으면 기준의 전달 메소드가 오류를 발생 시킵니다.\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=배치 크기(=1)\n",
    "    # b=특징 맵의 크기\n",
    "    # (c,d)=특징 맵(N=c*d)의 차원\n",
    "\n",
    "    features = input.view(a * b, c * d)  # F_XL을 \\hat F_XL로 크기 조정합니다\n",
    "\n",
    "    G = torch.mm(features, features.t())  # 그램 곱을 수행합니다\n",
    "\n",
    "    # 그램 행렬의 값을 각 특징 맵의 요소 숫자로 나누는 방식으로 '정규화'를 수행합니다.\n",
    "    return G.div(a * b * c * d)\n",
    "    \n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
